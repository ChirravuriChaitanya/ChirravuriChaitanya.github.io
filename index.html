
<!DOCTYPE html>
<html lang="en">
    <head>
	    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PC5YGB180G"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PC5YGB180G');
</script>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Venkata Chirravuri</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Venkata Chirravuri</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience"> Professional Experience</a></li>
		    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#projects">Academic Projects</a></li>
		    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#learnings">Self Learnings</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li>
		    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#volunteer">Volunteer Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">Interests</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Achievements & Certifications</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Venkata
                        <span class="text-primary">Chirravuri</span>
                    </h1>
                    <div class="subheading mb-5">
                        415 N Orchard St, Downingtown, PA 19341 · (517) 519-8090 ·
                        <a href="mailto:name@email.com">venkatachirravuri95@gmail.com</a>
                    </div>
                    <p class="lead mb-5">Highly skilled and technology-driven Computer Science graduate with 4+ years of experience translating business needs into effective software solutions. Proficient in cloud technologies for scalable deployments and adept at building ETL pipelines.</p>
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/venkata-chirravuri-ab322798/"><i class="fab fa-linkedin-in" aria-hidden="true"></i></a>
                        <a class="social-icon" href="https://github.com/ChirravuriChaitanya"><i class="fab fa-github" aria-hidden="true"></i></a>
                        <a href="https://drive.google.com/file/d/1lDHCiNS73a2iuPv3hUoZq98VeMKlpTcd/view?usp=sharing"><i>Download My Resume</i></a>
                       
                        <i ></i>
                        
                            <!--<i class="fab fa-twitter"></i></a>-->
                       <!-- <a class="social-icon" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5">Professional Experience</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">

				<h3 class="mb-0">Data Engineer</h3>
                            <div class="subheading mb-3">Amazon Contract - SID Global Solutions</div>
                            <p> <b> Analytics and Data Engineering - GEIST </b></p>
							<p>
							
							<ul>
   
    <li>Partnered closely with project managers, technical program managers (TPMs), and senior stakeholders to gather complex business and technical requirements, define clear project scopes, and deliver comprehensive, end-to-end reporting and analytics solutions.</li>
    <li>Maintained a strong focus on Amazon’s customer obsession, taking full ownership and acting with urgency to deliver timely, high-impact insights that improved executive decision-making and strengthened stakeholder trust across teams.</li>

    <li>Designed, architected, and implemented robust, highly scalable AWS solutions by integrating services such as Glue for ETL orchestration, Athena for flexible SQL querying, Lambda for serverless event automation, S3 for durable and cost-efficient storage, RDS and Redshift for structured warehousing, and SageMaker for advanced machine learning workloads. Ensured security, scalability, and cost optimization while aligning architectures with evolving business goals and best practices for data governance.</li>

    <li>Led cross-account data integration and consolidation initiatives spanning multiple AWS accounts across EU and NA regions. Designed and enforced secure IAM policies, cross-account roles, and automated Glue workflows to centralize fragmented data assets into a unified, governed environment. </li>
     <li>This initiative enabled enterprise-scale AI and analytics programs, improved company-wide data accessibility by 60%, reduced redundant storage costs, and ensured compliance with region-specific data privacy and governance requirements.</li>

    <li>Worked closely with BI analysts, data scientists, and business units to develop and deploy interactive, self-service dashboards using AWS QuickSight. Created reusable data models and optimized query layers to deliver real-time, high-quality, and reliable datasets, empowering business users with instant, actionable insights. </li>
    <li> These solutions significantly increased decision-making speed, enhanced reporting accuracy, and improved stakeholder satisfaction through faster turnaround times and visually rich analytics.</li>
</ul>






							<p> <b> <a>Resource Planning Tool 2.0</a></b></p>
							<p>
							<ul>
                                <li>Identified and resolved security gaps by redesigning the existing architecture to include robust cross-account access controls, optimizing AWS Glue jobs and the Glue Data Catalog to ensure data integrity and compliance, which resulted in a 75% reduction in security incidents and supported secure, automated data pipelines feeding into RDS.</li>
    <li>Extracted, cleaned, and transformed high-volume, complex data from multiple structured and unstructured sources such as relational databases, REST APIs, and server log files. </li>
	   <li> Built insights-ready datasets using Python, Spark, and AWS services to accelerate data availability for analytics and improve pipeline throughput by 40%.</li>
    <li>Developed, tested, and maintained reusable, modular Python scripts and highly reliable data pipelines for acquisition, validation, transformation, and modeling, cutting manual intervention by 50% and significantly improving data accuracy and processing consistency.</li>
    <li>Applied deep expertise in ETL best practices, data modeling, and data architecture principles to design robust data integration frameworks that were scalable, maintainable, and aligned with evolving business requirements and Agile development methodologies.</li>
    <li>Designed and optimized RDBMS structures and performance, including creating and fine-tuning tables, views, indexes, stored procedures, cursors, and triggers, which improved query execution times and reduced overall data processing time by 30%.</li>
    <li>Built and maintained end-to-end CI/CD pipelines using AWS CodePipeline, CodeCommit, CodeDeploy, and CloudFormation to automate deployment workflows, ensure robust version control, and enable rapid, reliable releases, cutting deployment time by 40% and improving development efficiency.</li>
    <li>Facilitated continuous knowledge sharing and collaboration across project teams, ensuring alignment with security standards, coding best practices, and AWS architectural guidelines to deliver high-quality, scalable solutions that consistently met business SLAs.</li>
 </ul>
			</div>
			     <div class="flex-shrink-0"><span class="text-primary">January 2024 - Current</span></div>
		    </div>
<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
<h3 class="mb-0">Data Engineer</h3>
                            <div class="subheading mb-3">Tech Mahindra</div>
                            <p> <b> Tech Mahindra’s Data Engineering Excellence with AWS </b></p>
							<p>
							<ul>
                                <li>Orchestrated diverse dataset ingestion into Amazon S3, ensuring data durability, scalability, easy access, and data storage capabilities.</li>
                                <li>Utilized AWS Glue for ETL pipelines, to effectively transform raw data from S3  into structured, query-ready formats.</li>
                                <li>Employed Lambda function to trigger Glue jobs for every data inclusive event in S3 bucket and store in Snowflake as DataWarehouse.</li>
                                <li>Created Snowflake data warehouse on AWS using Snowflake services, defined and configured data structures, schemas.</li>
                                <li>Additionally, employed Redshift for additional data warehouse to leverage AWS Athena, AWS Glue Catalog schema inference.</li>
                                <li>Enabled seamless schema inference for data analytics, reducing manual effort by 80% and improving data accuracy.</li>
                                <li>Leveraged AWS Athena for efficient SQL querying on the transformed data, providing on-demand data analysis and decision-making.</li>
                                <li>The developed pipeline has achieved a remarkable 40% enhancement in data processing speed, optimizing data for advanced analysis.</li>
                                <li>Created interactive & insightful visualizations using AWS QuickSight, to provide dashboards for stakeholders to derive valuable insights.</li>
                                <li>Executed end-to-end data engineering operations, orchestrating data ingestion, transformation, analysis, visualization and warehousing.</li>
                                <li>Proposed approach accelerated data processing time by 40%, resulting in faster insights for decision-making.</li></ul>





							<p> <b> <a href='https://www.techmahindra.com/en-in/data-and-analytics/data-management-udmf/'>Unified Data Migration Framework</a></b></p>
							<p>
							<ul>
                                <li>Developed efficient data migration libraries using Python on Spark for Verizon, resulting in UDMF 2.0 implementation.</li>
                                <li>Actively participated in streamlining various manual workflows by implementing Python scripts and Unix shell scripting for automation.</li>
                                <li>Re-organised Python scripts to optimize reusable libraries leveraging spark, leading to a 30% processing time reduction.</li>
                                <li>Improved system performance, reducing overhead costs by 50% through subqueries, stored procedures and triggers in SQL databases.</li>
                                <li>Migrated high-volume data, boosting data processing speed by 40% and reducing migration time by 50%.</li>
                                <li>Resolved critical errors, conducted rigorous debugging, resulting in a 40% reduction in system downtime, enhancing operational efficiency.</li>
                                <li>Skillfully maintained code versioning using GitHub for efficient collaboration and version control.</li>
                                <li>The framework has improved overall system performance and has demonstrated a 50% reduction in overhead costs.</li>
                                </ul>

<p> <b>Data Migration: Teradata - Google Big Query</b></p>
							<p>
							<ul>
                                <li>Brainstormed and proposed a proof-of-concept leveraging Python, GCP for Tech Mahindra and other business partners.</li>
                                <li>Facilitated seamless data migration pipelines from a database management system to a cloud-based data warehouse.</li>
                                <li>Led the end-to-end development cycle, including design, development, testing, deployment and maintenance of the application.</li>
                                <li>Upgraded Python scripts using Pandas, db-connectors  to automate hassle-free data migration from Teradata to Google BigQuery.</li>
                                <li>Achieved a 50% reduction in data transfer time to  enable faster data analysis and reporting.</li>
                                <li>Planned relevant metrics, objectives and constraints for data pre-processing techniques and data pipelines.</li>
                                <li>Documented the design documents, architecture documents, test plans, quality analysis and audits.</li></ul>

                            
                   
<!--                     <div class="d-flex flex-column flex-md-row justify-content-between mb-5"> -->
<!--                         <div class="flex-grow-1"> -->
<!--                             <h3 class="mb-0">Jr. Data Scientist</h3> -->
<!--                             <div class="subheading mb-3">Tech Mahindra</div>

 -->

<p><b><a href='chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://files.techmahindra.com/static/img/pdf/laila.pdf'>Artificial Intelligent Legal Assistant (AILA)</a></b></p>
							<p>
							<ul>
                                <li>Worked in a team to conceptualize and develop L’AILA, using Natural Language Processing (NLP) techniques.</li>
                                <li>L’AILA, an innovative application designed to automate the management of legal contracts and enable varied insights to the user.</li>
                                <li>Managed the preprocessing and analysis of a substantial dataset comprising 7000 legal contracts.</li>
                                <li>Designed Python scripts to transform the processed data into JSON format and subsequently inserted into MongoDB database.</li>
                                <li>Modeled a  bi-directional Long Short-Term Memory (LSTM) text classifiers using the TensorFlow on the processed data.</li>
                                <li>Took on a leadership role, overseeing a team that included legal experts/developers to gather client requirements and expectations.</li>
                                <li>Devised and implemented strategies for a rule-based Named Entity Recognition (NER) approach, achieving 94% accuracy rate.</li>
                                <li>Implementation of L’AILA had a tangible impact on workflow efficiency, marking a substantial improvement in operational efficiency.</li>
                                <li>Reduced the contract turnaround time from a laborious 10 days to a mere 7 hours. </li>
                                <li>Enhanced reliability of analysis by minimizing manual errors, resulting in a 40% reduction in discrepancies, ensuring contract compliance.</li>
                                </ul>



                            
                        </div>
				
<!--                             <h3 class="mb-0">Data Engineer</h3>
                            <div class="subheading mb-3">Tech Mahindra</div>
                            <p> <b> Tech Mahindra’s Data Engineering Excellence with AWS </b></p>
							<p>
							<ul>
                                <li>Orchestrated diverse dataset ingestion into Amazon S3, ensuring data durability, scalability, easy access, and data storage capabilities.</li>
                                <li>Utilized AWS Glue for ETL pipelines, to effectively transform raw data from S3  into structured, query-ready formats.</li>
                                <li>Employed Lambda function to trigger Glue jobs for every data inclusive event in S3 bucket and store in Snowflake as DataWarehouse.</li>
                                <li>Created Snowflake data warehouse on AWS using Snowflake services, defined and configured data structures, schemas.</li>
                                <li>Additionally, employed Redshift for additional data warehouse to leverage AWS Athena, AWS Glue Catalog schema inference.</li>
                                <li>Enabled seamless schema inference for data analytics, reducing manual effort by 80% and improving data accuracy.</li>
                                <li>Leveraged AWS Athena for efficient SQL querying on the transformed data, providing on-demand data analysis and decision-making.</li>
                                <li>The developed pipeline has achieved a remarkable 40% enhancement in data processing speed, optimizing data for advanced analysis.</li>
                                <li>Created interactive & insightful visualizations using AWS QuickSight, to provide dashboards for stakeholders to derive valuable insights.</li>
                                <li>Executed end-to-end data engineering operations, orchestrating data ingestion, transformation, analysis, visualization and warehousing.</li>
                                <li>Proposed approach accelerated data processing time by 40%, resulting in faster insights for decision-making.</li></ul>





							<p> <b> <a href='https://www.techmahindra.com/en-in/data-and-analytics/data-management-udmf/'>Unified Data Migration Framework</a></b></p>
							<p>
							<ul>
                                <li>Developed efficient data migration libraries using Python on Spark for Verizon, resulting in UDMF 2.0 implementation.</li>
                                <li>Actively participated in streamlining various manual workflows by implementing Python scripts and Unix shell scripting for automation.</li>
                                <li>Re-organised Python scripts to optimize reusable libraries leveraging spark, leading to a 30% processing time reduction.</li>
                                <li>Improved system performance, reducing overhead costs by 50% through subqueries, stored procedures and triggers in SQL databases.</li>
                                <li>Migrated high-volume data, boosting data processing speed by 40% and reducing migration time by 50%.</li>
                                <li>Resolved critical errors, conducted rigorous debugging, resulting in a 40% reduction in system downtime, enhancing operational efficiency.</li>
                                <li>Skillfully maintained code versioning using GitHub for efficient collaboration and version control.</li>
                                <li>The framework has improved overall system performance and has demonstrated a 50% reduction in overhead costs.</li>
                                </ul>

<p> <b>Data Migration: Teradata - Google Big Query</b></p>
							<p>
							<ul>
                                <li>Brainstormed and proposed a proof-of-concept leveraging Python, GCP for Tech Mahindra and other business partners.</li>
                                <li>Facilitated seamless data migration pipelines from a database management system to a cloud-based data warehouse.</li>
                                <li>Led the end-to-end development cycle, including design, development, testing, deployment and maintenance of the application.</li>
                                <li>Upgraded Python scripts using Pandas, db-connectors  to automate hassle-free data migration from Teradata to Google BigQuery.</li>
                                <li>Achieved a 50% reduction in data transfer time to  enable faster data analysis and reporting.</li>
                                <li>Planned relevant metrics, objectives and constraints for data pre-processing techniques and data pipelines.</li>
                                <li>Documented the design documents, architecture documents, test plans, quality analysis and audits.</li></ul>


			    <p><b><a href='chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://files.techmahindra.com/static/img/pdf/laila.pdf'>Artificial Intelligent Legal Assistant (AILA)</a></b></p>
							<p>
							<ul>
                                <li>Worked in a team to conceptualize and develop L’AILA, using Natural Language Processing (NLP) techniques.</li>
                                <li>L’AILA, an innovative application designed to automate the management of legal contracts and enable varied insights to the user.</li>
                                <li>Managed the preprocessing and analysis of a substantial dataset comprising 7000 legal contracts.</li>
                                <li>Designed Python scripts to transform the processed data into JSON format and subsequently inserted into MongoDB database.</li>
                                <li>Modeled a  bi-directional Long Short-Term Memory (LSTM) text classifiers using the TensorFlow on the processed data.</li>
                                <li>Took on a leadership role, overseeing a team that included legal experts/developers to gather client requirements and expectations.</li>
                                <li>Devised and implemented strategies for a rule-based Named Entity Recognition (NER) approach, achieving 94% accuracy rate.</li>
                                <li>Implementation of L’AILA had a tangible impact on workflow efficiency, marking a substantial improvement in operational efficiency.</li>
                                <li>Reduced the contract turnaround time from a laborious 10 days to a mere 7 hours. </li>
                                <li>Enhanced reliability of analysis by minimizing manual errors, resulting in a 40% reduction in discrepancies, ensuring contract compliance.</li>
                                </ul>
 -->
                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2018 - June 2021</span></div>
                    </div>

		    
<!--                     <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Jr. Data Scientist</h3>
                            <div class="subheading mb-3">Tech Mahindra</div>



<p><b><a href='chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://files.techmahindra.com/static/img/pdf/laila.pdf'>Artificial Intelligent Legal Assistant (AILA)</a></b></p>
							<p>
							<ul>
                                <li>Worked in a team to conceptualize and develop L’AILA, using Natural Language Processing (NLP) techniques.</li>
                                <li>L’AILA, an innovative application designed to automate the management of legal contracts and enable varied insights to the user.</li>
                                <li>Managed the preprocessing and analysis of a substantial dataset comprising 7000 legal contracts.</li>
                                <li>Designed Python scripts to transform the processed data into JSON format and subsequently inserted into MongoDB database.</li>
                                <li>Modeled a  bi-directional Long Short-Term Memory (LSTM) text classifiers using the TensorFlow on the processed data.</li>
                                <li>Took on a leadership role, overseeing a team that included legal experts/developers to gather client requirements and expectations.</li>
                                <li>Devised and implemented strategies for a rule-based Named Entity Recognition (NER) approach, achieving 94% accuracy rate.</li>
                                <li>Implementation of L’AILA had a tangible impact on workflow efficiency, marking a substantial improvement in operational efficiency.</li>
                                <li>Reduced the contract turnaround time from a laborious 10 days to a mere 7 hours. </li>
                                <li>Enhanced reliability of analysis by minimizing manual errors, resulting in a 40% reduction in discrepancies, ensuring contract compliance.</li>
                                </ul> -->



<!--                             
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2018 - March 2019</span></div>
<!--                     </div> -->     
<!--                         <div class="flex-shrink-0"><span class="text-primary">April 2019 - June 2021</span></div> -->
                    </div> 
</div>
			     <div class="flex-shrink-0"><span class="text-primary">August 2018 - June 2021</span></div>
		    </div>

	    
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Research Intern</h3>
                            <div class="subheading mb-3">Bhabha Atomic Research Centre</div>
<ul>
<li><a href='https://github.com/ChirravuriChaitanya/Language-Modelling'>Language Identification Model </a> : I have proposed and developed a machine learning model using n-gram,
Cumulative Frequency Addition Classifier (CFAC) and Naive Bayesian techniques to identify the language text.</li>
<li>The novel work is identifying the roman-scripted Hindi language as itself and I was able to achieve a high accuracy
of 98% using Leipzig Corpora and Twitter Datasets.</li></ul>
                           
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2017 - July 2017</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Web Design Intern</h3>
                            <div class="subheading mb-3">Tata Consultancy Services</div>
                            <p>Plot Allotment- APCRDA: I have worked in team and developed a website for ’Plot Allotment Process’ division for Government of Andhra Pradesh under the Andhra Pradesh Capital Region Development Act (APCRDA). Used
HTML/CSS and JavaScript to make it more interactive and robust.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2016 - July 2016</span></div>
                    </div>
                </div>
            </section>

	 <hr class="m-0" />
	 <!--Projects-->
<section class="resume-section" id="projects">
                <div class="resume-section-content">
                    <h2 class="mb-5">Academic Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"><a href='https://comic-xr.github.io/'>Collaborative Immersive Analytics</a></h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Designed and extended the immersive functionality into Virtual Reality space in Oculus Quest 2 devices.</li>
							<li>Developed a collaborative environment for users to communicate and interact with the game objects.</li>
							<li>Utilized Photon Unity Networking to enable multi-user functionality and make interactions easier.</li>
<li>Contributing to the greater building of CoMic- A Collaborative Mobile Immersive Computing Infrastructure for
conducting Multi-user XR research, an ongoing development of research infrastructure.</li>
<li>Best Project Award (2/30) –Developing Collaborative Immersive Analytics powered by IATK on Oculus Quest 2.</li></ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>





<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Car Price Prediction</h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Took lead in designing and developing a precise approach for estimating used car prices, leveraging data from Craigslist.
</li>
<li>Employed PySpark & MLlib for efficient data processing, cutting processing time by 60% and boosting predictive accuracy by 45%. Created Power BI dashboards for comprehensive analysis of multiple machine learning models’ results.</li>
							</ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>








<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Cost Sensitive Analysis</h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Applied different classifiers on a dataset picked up from KDD-98 challenge of a national veteran organization to
                            identify the ways to minimize the marketing costs by identifying the plausible donors and predict their donations to make organizations profitable.</li>
							<li>I have used classification models like Logistic Regression, Random Forest, Decision Tree and Balanced Bagging
                            Classifier and compared the results of these different classifiers.</li>
							</ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>








<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"><a href='https://github.com/ChirravuriChaitanya/SWE_642'>Student Survey Portal</a></h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Gained expertise in full-stack development, leveraging AWS services (S3, EC2, IAM) for application creation and hosting.</li>
							<li>Employed Docker and Kubernetes for reliable and scalable application containerization and deployment, in Rancher-managed K8 clusters. Enhanced product updates speed and reliability by setting up CI/CD pipelines with Jenkins.</li></ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>





<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Topic Modelling</h3>
                            <div class="subheading mb-3">Mahindra University</div>
							
							<p>
							<ul>
							<li>
							Led a team in developing a machine learning model using Latent Dirchilet Allocaton (LDA) for Document
Clustering, Exploration, and theme extraction.</li>
							<li>Explored and implemented several data cleaning and data pre-processing processes by implementing root-and-rule
pre-processing techniques. The model has achieved an impressive accuracy of 97%.</li></ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>





                    </div>
                </div>
            </section>



            <hr class="m-0" />
            <!-- Learnings-->
            <section class="resume-section" id="learnings">
                <div class="resume-section-content">
                    <h2 class="mb-5">Self Learnings</h2>





                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Data Engineer Learnings</h3>









                            






                        <br>
                        <div class="subheading mb-3"><b>Real-Time Stock Market Analysis Project using AWS services  and Apache Kafka</b></div>
                           <p>
                            <ul>

                         
                            <li><b>Data Acquisition from Kaggle:</b>
                            <ul>
                                <li>Obtained the Stock Market Data from Kaggle. This dataset likely includes historical stock price and trading volume data for various stocks and time periods.</li>
                                

                            </ul></li>
                            
                               <li> <b>Data Transformation and Real-Time Data Updates:</b>
                            <ul>

                                <li>Developed Python scripts to preprocess the Kaggle data and transform it into a format suitable for real-time updates.</li>
                                <li>This involved data cleaning, formatting, and structuring.</li>
                                <li>Set up a mechanism to continuously update the data in real-time, ensuring that new stock market data is ingested as it becomes available.</li>
                        
                            </ul></li>

                            
                            <li><b>Apache Kafka Integration:</b>
                         <ul>

                             <li>Downloaded Apache Kafka in EC2 instance, initialized it and used Apache-kafka-Python libraries to communicate with Apache Kafka.</li>
                             <li>Integrated Apache Kafka into the data pipeline. Kafka can serve as a messaging system to handle real-time data streams.</li>
                             <li>Configured Kafka producers to send the transformed stock market data in JSON format to Kafka topics in Kafka broker, where it can be consumed by downstream processes.</li>
							 <li>Configured Kafka Consumers to consume the data and load the JSON data onto S3 bucket.</li>
                     
                         </ul>

                      
                           <li> <b>AWS S3 Data Ingestion:</b>
                         <ul>

                             <li>Set up an AWS S3 bucket to store the data.</li>
                             <li>Developed Python scripts using S3FS to send the real-time stock market data to the appropriate S3 location. This is done by configuring Kafka consumers to write data to S3.</li>
                         
                     
                         </ul></li>

                      
                         <li><b>AWS Glue Crawler Configuration:</b>
                      <ul>

                          <li>Configured AWS Glue Crawlers to automatically discover and catalog metadata about the data stored in S3 buckets. This included inferring the schema and table structure of the data.</li>
                                             
                  
                      </ul></li>

                     
                         <li><b>AWS Glue Data Catalog:</b>
                      <ul>

                          <li>Utilized the AWS Glue Data Catalog as the centralized metadata repository for the data. This catalog contained information about the stock market data tables, their schemas, and locations in S3.</li>
                                              
                  
                      </ul></li>

                      
                    
                         <li><b>AWS Athena Integration:</b>
                      <ul>

                          <li>Set up AWS Athena to query the data stored in S3 buckets. Athena can use the metadata stored in the AWS Glue Data Catalog to facilitate SQL querying.</li>
                          <li>Defined and executed SQL queries to analyze the stock market data.</li>
                          <li>Generated reports, visualizations, and insights from the data using Athena.</li>
                      
                  
                      </ul></li>
                      <br>
                    


                        <br>
                        <div class="subheading mb-3"><b>Youtube Analysis Project using AWS </b></div>
                           <p><i>To be Uploaded Soon</i></p>

                        <br>
                        <br>
                        <div class="subheading mb-3"><b>Data migration from on-prem database to Azure Cloud</b></div>
                           <p>
                            <ul>

                         
                            <li><b>Leveraged Azure stack for ETL Pipelines and Analytics:</b>
                            <ul>
                                <li>Utilized a suite of Azure services, including Azure Data Factory, Databricks, Data Lake Storage Gen2, Synapse Analytics, Key Vault, Active Directory, and PowerBI.</li>
                                <li>Employed these services to design, develop, and execute end-to-end ETL (Extract, Transform, Load) pipelines and conducted advanced analytics on data.</li>

                            </ul></li>
                            
                               <li> <b>Resource Group Management and Integration:</b>
                            <ul>

                                <li>Created and managed resource groups to logically organize and streamline Azure resources for efficient administration.</li>
                                <li>Configured self-hosted runtime integrations, enabling seamless communication between Azure resources and an on-premise database.</li>
                                <li>Facilitated smooth data flow between cloud and on-premises environments.</li>
                        
                            </ul></li>

                            
                            <li><b>Data Extraction and Storage with Azure Data Factory (ADF):</b>
                         <ul>

                             <li>Designed and implemented data pipelines and activities within Azure Data Factory (ADF).</li>
                             <li>Leveraged ADF to efficiently extract specific tables and datasets from the on-premise database.</li>
                             <li>Stored extracted data in Azure Data Lake Storage Gen2, ensuring accessibility and data integrity.</li>
                     
                         </ul>

                      
                           <li> <b>Data Transformation with Databricks:</b>
                         <ul>

                             <li>Utilized Azure Databricks for comprehensive data transformation.</li>
                             <li>Processed raw, unstructured data from diverse sources, transforming it into curated, high-quality data suitable for advanced analytics.</li>
                         
                     
                         </ul></li>

                      
                         <li><b>Data Loading with Azure Synapse Analytics:</b>
                      <ul>

                          <li>Employed Azure Synapse Analytics for data loading and management.</li>
                          <li>Loaded clean and transformed data into Azure Synapse Analytics, preparing it for in-depth analytical queries and reporting.</li>
                      
                  
                      </ul></li>

                     
                         <li><b>Interactive Dashboard Creation with PowerBI:</b>
                      <ul>

                          <li>Utilized PowerBI to craft interactive, user-friendly dashboards.</li>
                          <li>Enabled users to explore data visually and derive valuable insights through rich and intuitive data visualization.</li>
                      
                  
                      </ul></li>

                      
                    
                         <li><b>Active Directory and Key Vault for Security and Governance:</b>
                      <ul>

                          <li>Implemented Azure Active Directory for robust access control and authentication, ensuring secure interactions with Azure resources.</li>
                          <li>Utilized Azure Key Vault to securely store and manage sensitive credentials and secrets. </li>
                          <li>These measures enhanced data security and supported comprehensive monitoring and data governance practices.</li>
                      
                  
                      </ul></li>






                    </ul>
                           </p>
							

                            
                        </div>


                       









                       
                    </div>






                    <br>
                        <div class="subheading mb-3"><b>Building ETL (Extract, Transform, Load) pipelines for Tokyo Olympic Datasets</b></div>
                        <p><a href="https://github.com/ChirravuriChaitanya/tokyo-olympic-azure-data-engineering-project">View Project</a></p>
                        <p>
                            <ul>

                         
                            <li><b>Learning Azure Tools for Tokyo Olympic Data ETL:</b>
                            <ul>
                                <li>Acquired proficiency in Azure tools including Azure Data Factory, Databricks, Data Lake, and Synapse for the purpose of building ETL (Extract, Transform, Load) pipelines.</li>
                                <li>Applied this knowledge to work with Tokyo Olympic data, demonstrating the practical application of these tools.</li>

                            </ul></li>
                            
                               <li> <b>Resource Group Management and Data Extraction:</b>
                            <ul>

                                <li>Established and managed resource groups in Azure for effective organization and resource management.</li>
                                <li>Configured self-hosted runtime integrations, enabling seamless communication between Azure resources and an on-premise database.</li>
                                <li>Demonstrated expertise in handling external data sources, ensuring data retrieval efficiency.</li>
                        
                            </ul></li>

                            
                            <li><b>Data Extraction and Storage with Azure Data Factory (ADF):</b>
                         <ul>

                             <li>Leveraged Azure Data Factory (ADF) to create pipelines and activities for extracting specific tables or datasets from the source database.</li>
                             <li>Utilized ADF as a key component in the data extraction process, ensuring seamless data transfer.</li>
                             <li>Stored the extracted data in Azure Data Lake Storage Gen2, providing a secure and scalable storage solution.</li>
                     
                         </ul>

                      
                           <li> <b>Data Transformation with Databricks:</b>
                         <ul>

                             <li>Harnessed the power of Azure Databricks, employing PySpark for data transformation.</li>
                             <li>Transformed raw data into cleaner, well-structured data, enhancing data quality and readiness for analysis.</li>
                             <li>Demonstrated proficiency in utilizing Databricks for data wrangling and refinement.</li>
                         
                     
                         </ul></li>

                      
                         <li><b>Data Loading with Azure Synapse Analytics:</b>
                      <ul>

                          <li>Utilized Azure Synapse Analytics for data loading and management, ensuring efficient data ingestion.</li>
                          <li>Loaded the clean and transformed data into the database within Azure Synapse Analytics.</li>
                          <li>Leveraged Synapse Analytics for query analytics, enabling the creation of visual graphs and charts for data visualization.</li>
                          <li>Demonstrated the ability to derive valuable insights through query analytics.</li>
                      
                  
                      </ul></li>             
                        </ul>
                           </p>

                           
							

                           <br>
                           <div class="subheading mb-3"><b>Data Modelling</b></div>
                           <p><a href="https://github.com/ChirravuriChaitanya/DataModelling">View Project</a></p>
                           <p>
                            <ul>

                         
                            <li><b>Overview and Dataset Selection:</b>
                            <ul>
                                <li>The project revolves around the analysis of the dvdrental dataset, which is readily available on the PostgreSQL website.</li>
                                <li>The first step involves downloading this dataset and preparing it for further processing.</li>

                            </ul></li>
                            
                               <li> <b>Data Loading into PostgreSQL:</b>
                            <ul>

                                <li>Following dataset selection, the data is downloaded and loaded into a PostgreSQL database.</li>
                                <li>This process ensures that the dataset is accessible and can be manipulated for subsequent analysis.</li>
                                                       
                            </ul></li>

                            
                            <li><b>Connecting Python to PostgreSQL with psycopg2:</b>
                         <ul>

                             <li>To work with the data effectively, the psycopg2 library is employed to establish a connection between Python and PostgreSQL.</li>
                             <li>This connection facilitates seamless interaction with the database from within Python.</li>
                            
                     
                         </ul>

                      
                           <li> <b>Data Model Design in STAR Schema:</b>
                         <ul>

                             <li>A pivotal part of the project is the design of a data model. In this case, a relational modeling approach is chosen.</li>
                             <li>The goal is to restructure the dataset into a STAR Schema, a common data warehousing model that simplifies data retrieval and analysis.</li>
                                                    
                     
                         </ul></li>

                      
                         <li><b>Using psycopg2 for Database Operations:</b>
                      <ul>

                          <li>The psycopg2 library is instrumental in this process and is used for various tasks.</li>
                          <li>Key actions include initializing the connection, creating the database, and initializing a cursor to execute queries.</li>
                                              
                  
                      </ul></li>
                      <li> <b>Query Execution and Database Operations:</b>

                      <ul>

                        <li>The initialized cursor is employed to execute SQL queries, perform data manipulations, and carry out operations on the database.</li>
                        <li>This phase involves crafting and executing queries to transform and restructure the dataset as per the STAR Schema design.</li>
                                               
                
                    </ul></li>

                 
                    <li><b>Closing Cursor and Connection:</b>
                 <ul>

                     <li>After the necessary database operations are completed, it's essential to close the cursor and connection.</li>
                     <li>This ensures efficient resource management and proper termination of the database connection.</li>
                                         
             
                 </ul></li>

                     
                        
                    </ul>
                           </p>  
                           <br>
                           <br>
                           <br>




                           <h3 class="mb-0">Full Stack Learnings</h3>
                           <p>Next Coming Up! <b>Blog Clone Project</b></p>










                        </div>
          

                       
                    </div>
          
                     

                     
                 </div>



                    
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            
                            
                    </div>
                </div>
            </section>






            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">George Mason University</h3>
                            <div class="subheading mb-3">Master of Science</div>
                            <div>Computer Science </div>
                            <p>GPA: 3.40</p>
			<p><b> CourseWork:</b> Theory/Application Data Mining, Mining Mass Datasets MapReduce, Software Model/Architechural Deisgn, Software Engineering for World Wide Web, Mobile Immersive Computing, Database Systems, Computer Systems and System Programming, Component-Based Software Development, Mathematical Foundations of CS, Analysis of Algorithms I</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2021 - May 2023</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Mahindra University</h3>
                            <div class="subheading mb-3">Bachelor of Science</div>
			 <div>Computer Science </div>
                            <p>GPA: 7.29</p>
		<p><b> CourseWork:</b> Artificial Intelligence, Cloud Computing, 
Computer Architectural Design, Computer Communications Networks, Cryptography & Information Security, 
Data Structures & Algorithms, Mobile Communications & Computing, Object Oriented Programming Using Java, 
Operating Systems, 
Entrepreneurship, IPR & LAW, Design Thinking, 
Corporate Management & Finance, Cinema & Philosophy </p>
				
			
                        </div>

                        <div class="flex-shrink-0"><span class="text-primary">August 2014 - May 2018</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Skills-->
            <section class="resume-section" id="skills">
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <p><ul><li><b>Programming Languages:</b> Python, Java, JavaScript, SQL, HTML/CSS </li>
<li><b>Databases :</b> MySQL, PostgreSQL, MongoDB, Oracle, Snowflake, Cassandra</li>
<li><b>Big Data Technologies : </b> Hadoop, HDFS-MapReduce, Hive, Spark, PySpark, Spark Streaming</li>
<li><b> Frameworks :</b>Apache Kafka, Airflow, Django, Flask, Fast API, Rest API, SSIS, AngularJS </li>
<li><b>Cloud Technologies :</b>  AWS (EC2, S3, EMR, Lambda, Athena, DynamoDB RDS, RedShift, SNS, SQS, CloudWatch, Glue, Kinesis)<br/>
                               &emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &ensp; Azure(Data Factory, Data Lake, Lake House, Databricks, Key Vault, Active Directory)<br/>
                               &emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &ensp;GCP Big Query
</li>
<li><b>Visualization Tools & CI/CD :</b>Power BI, Matplotlib, Tableau, Quick Sight, Looker, Docker, Git, Kubernetes</li></ul></p>
                    <div class="subheading mb-3">Workflow</div>
                    <ul class="fa-ul mb-0">
                       
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cross Functional Teams
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Agile Development & Scrum
                        </li>
			<li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Kanban
                        </li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />

  <!-- Volunteer Experience-->
            <section class="resume-section" id="volunteer">
                <div class="resume-section-content">
                    <h2 class="mb-5">Volunteer Experience</h2>
                    <p><ul>
			<li>Organized inter-college fests 'Aether' at Mahindra University</li>
			<li>Active member for a NGO to empower women safety</li></ul></p>
                   
                </div>
            </section>
            <hr class="m-0" />

            <!-- Interests-->
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>Apart from being a computer science enthuisiast, I enjoy most of my time being outdoors. I enjoy playing basketball, badminton and running.</p>
                    <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements across the world.</p>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Achievements & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Tableau 2022 A-Z: Hand-On Tableau Training for Data Science by Udemy
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            AWS Certified Data Analytics Specialty 2023 by Udemy
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Hacker Rank Certification in Python
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - George Mason University - Best Project in Mobile Immersive Computing
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - Mahindra University - Deans List 2017
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Mahindra University - Deans List 2016
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>nd</sup>
                            Scholarship receipent for Outstanding Performance - Bhabha Atomic Research Centre
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Distinction - Australian Chemistry - 2010
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Distinction - Australian Chemistry - 2009
                        </li>
                    </ul>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
