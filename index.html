
<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Venkata Chirravuri</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Venkata Chirravuri</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#about">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience"> Professional Experience</a></li>
		    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#projects">Academic Projects</a></li>
		    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#learnings">Self Learnings</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#skills">Skills</a></li>
		    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#volunteer">Volunteer Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#interests">Interests</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Achievements & Certifications</a></li>
                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="about">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Venkata
                        <span class="text-primary">Chirravuri</span>
                    </h1>
                    <div class="subheading mb-5">
                        4259 Cotswolds Hill Ln, Fairfax, VA 22030 · (517) 519-8090 ·
                        <a href="mailto:name@email.com">venkatachirravuri95@gmail.com</a>
                    </div>
                    <p class="lead mb-5">Highly skilled and technology-driven Computer Science graduate with 3 years of experience translating business needs into effective software solutions. Proficient in cloud technologies for scalable deployments and adept at building ETL pipelines.</p>
                    <div class="social-icons">
                        <a class="social-icon" href="https://www.linkedin.com/in/venkata-chirravuri-ab322798/"><i class="fab fa-linkedin-in" aria-hidden="true"></i></a>
                        <a class="social-icon" href="https://github.com/ChirravuriChaitanya"><i class="fab fa-github" aria-hidden="true"></i></a>
                        <a href="https://drive.google.com/file/d/1E1tCyTnZHqaDcNn3sCxY5vrssSCp5YId/view?usp=sharing"><i>Download My Resume</i></a>
                       
                        <i ></i>
                        
                            <!--<i class="fab fa-twitter"></i></a>-->
                       <!-- <a class="social-icon" href="#!"><i class="fab fa-facebook-f"></i></a>-->
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Experience-->
            <section class="resume-section" id="experience">
                <div class="resume-section-content">
                    <h2 class="mb-5">Professional Experience</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Data Engineer</h3>
                            <div class="subheading mb-3">Tech Mahindra</div>
							<p> <b> <a href='https://www.techmahindra.com/en-in/data-and-analytics/data-management-udmf/'>Unified Data Migration Framework</a></b></p>
							<p>
							<ul>
							<li>
							Developed efficient data migration libraries leveraging Spark, resulting in a successful implementation of UDMF 2.0 for Verizon;migrated high volume data, enhancing data processing speed by 40% and reducing migration time by 50%.</li>
							<li> Optimized native reusable libraries functionalities by implementing Python, Scala, and SQL techniques, resulting in a 40% reduction in processing time.</li>
							<li>The framework has improved overall system performance and has demonstrated a 50% reduction in overhead costs.</li></ul>

<p> <b>Data Migration: Teradata - Google Big Query</b></p>
							<p>
							<ul>
							<li>
							Brainstormed and proposed a proof-of-concept for Tech Mahindra and other business partners, facilitating seamless data migration from a database management system to a cloud-based data warehouse.</li>
							<li> Created Python scripts to automate hassle-free data migration from Teradata to Google BigQuery, resulting in a 50% reduction in data transfer time and enabling faster data analysis and reporting.</li>
							<li>Planned relevant metrics, objectives and constraints for data pre-processing techniques and data pipelines.</li></ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Jr. Data Scientist</h3>
                            <div class="subheading mb-3">Tech Mahindra</div>



<p><b><a href='chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://files.techmahindra.com/static/img/pdf/laila.pdf'>Artificial Intelligent Legal Assistant (AILA)</a></b></p>
							<p>
							<ul>
							<li>
							Collaborated in a team to develop AILA, an application to automate legal contracts management using NLP techniques. </li>
<li> Preprocessed and analyzed 7000 contracts to train and test bi-directional LSTM text classifiers.
</li>
<li>Led a team, including Legal experts, analyzed strategies for rule-based NER approach to extract provisions, metadata, and risks with 94% accuracy.</li>
<li>The platform reduces turnaround time from 10 days to 7 hours and increases accuracy by minimizing manual errors.

</li></ul>



                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2018 - December 2019</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Research Intern</h3>
                            <div class="subheading mb-3">Bhabha Atomic Research Centre</div>
<ul>
<li><a href='https://github.com/ChirravuriChaitanya/Language-Modelling'>Language Identification Model </a> : I have proposed and developed a machine learning model using n-gram,
Cumulative Frequency Addition Classifier (CFAC) and Naive Bayesian techniques to identify the language text.</li>
<li>The novel work is identifying the roman-scripted Hindi language as itself and I was able to achieve a high accuracy
of 98% using Leipzig Corpora and Twitter Datasets.</li></ul>
                           
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2017 - July 2017</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Web Design Intern</h3>
                            <div class="subheading mb-3">Tata Consultancy Services</div>
                            <p>Plot Allotment- APCRDA: I have worked in team and developed a website for ’Plot Allotment Process’ division for Government of Andhra Pradesh under the Andhra Pradesh Capital Region Development Act (APCRDA). Used
HTML/CSS and JavaScript to make it more interactive and robust.</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">May 2016 - July 2016</span></div>
                    </div>
                </div>
            </section>

	 <hr class="m-0" />
	 <!--Projects-->
<section class="resume-section" id="projects">
                <div class="resume-section-content">
                    <h2 class="mb-5">Academic Projects</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"><a href='https://comic-xr.github.io/'>Collaborative Immersive Analytics</a></h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Designed and extended the immersive functionality into Virtual Reality space in Oculus Quest 2 devices.</li>
							<li>Developed a collaborative environment for users to communicate and interact with the game objects.</li>
							<li>Utilized Photon Unity Networking to enable multi-user functionality and make interactions easier.</li>
<li>Contributing to the greater building of CoMic- A Collaborative Mobile Immersive Computing Infrastructure for
conducting Multi-user XR research, an ongoing development of research infrastructure.</li>
<li>Best Project Award (2/30) –Developing Collaborative Immersive Analytics powered by IATK on Oculus Quest 2.</li></ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>





<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Car Price Prediction</h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Took lead in designing and developing a precise approach for estimating used car prices, leveraging data from Craigslist.
</li>
<li>Employed PySpark & MLlib for efficient data processing, cutting processing time by 60% and boosting predictive accuracy by 45%. Created Power BI dashboards for comprehensive analysis of multiple machine learning models’ results.</li>
							</ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>








<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Cost Sensitive Analysis</h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Applied different classifiers on a dataset picked up from KDD-98 challenge of a national veteran organization to
                            identify the ways to minimize the marketing costs by identifying the plausible donors and predict their donations to make organizations profitable.</li>
							<li>I have used classification models like Logistic Regression, Random Forest, Decision Tree and Balanced Bagging
                            Classifier and compared the results of these different classifiers.</li>
							</ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>








<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0"><a href='https://github.com/ChirravuriChaitanya/SWE_642'>Student Survey Portal</a></h3>
                            <div class="subheading mb-3">George Mason University</div>
							
							<p>
							<ul>
							<li>
							Gained expertise in full-stack development, leveraging AWS services (S3, EC2, IAM) for application creation and hosting.</li>
							<li>Employed Docker and Kubernetes for reliable and scalable application containerization and deployment, in Rancher-managed K8 clusters. Enhanced product updates speed and reliability by setting up CI/CD pipelines with Jenkins.</li></ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>





<div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Topic Modelling</h3>
                            <div class="subheading mb-3">Mahindra University</div>
							
							<p>
							<ul>
							<li>
							Led a team in developing a machine learning model using Latent Dirchilet Allocaton (LDA) for Document
Clustering, Exploration, and theme extraction.</li>
							<li>Explored and implemented several data cleaning and data pre-processing processes by implementing root-and-rule
pre-processing techniques. The model has achieved an impressive accuracy of 97%.</li></ul>

                            
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2019 - June 2021</span></div>
                    </div>





                    </div>
                </div>
            </section>



            <hr class="m-0" />
            <!-- Learnings-->
            <section class="resume-section" id="learnings">
                <div class="resume-section-content">
                    <h2 class="mb-5">Self Learnings</h2>





                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Data Engineer Learnings</h3>









                            






                        <br>
                        <div class="subheading mb-3"><b>Real-Time Stock Market Analysis Project using AWS services </b></div>
                           <p>
                            <ul>

                         
                            <li><b>Data Acquisition from Kaggle:</b>
                            <ul>
                                <li>Obtained the Stock Market Data from Kaggle. This dataset likely includes historical stock price and trading volume data for various stocks and time periods.</li>
                                

                            </ul></li>
                            
                               <li> <b>Data Transformation and Real-Time Data Updates:</b>
                            <ul>

                                <li>Developed Python scripts to preprocess the Kaggle data and transform it into a format suitable for real-time updates.</li>
                                <li>This involved data cleaning, formatting, and structuring.</li>
                                <li>Set up a mechanism to continuously update the data in real-time, ensuring that new stock market data is ingested as it becomes available.</li>
                        
                            </ul></li>

                            
                            <li><b>Apache Kafka Integration:</b>
                         <ul>

                             <li>Downloaded Apache Kafka in EC2 instance, initialized it and used Apache-kafka-Python libraries to communicate with Apache Kafka.</li>
                             <li>Integrated Apache Kafka into the data pipeline. Kafka can serve as a messaging system to handle real-time data streams.</li>
                             <li>Configured Kafka producers to send the transformed stock market data in JSON format to Kafka topics in Kafka broker, where it can be consumed by downstream processes.</li>
							 <li>Configured Kafka Consumers to consume the data and load the JSON data onto S3 bucket.</li>
                     
                         </ul>

                      
                           <li> <b>AWS S3 Data Ingestion:</b>
                         <ul>

                             <li>Set up an AWS S3 bucket to store the data.</li>
                             <li>Developed Python scripts using S3FS to send the real-time stock market data to the appropriate S3 location. This is done by configuring Kafka consumers to write data to S3.</li>
                         
                     
                         </ul></li>

                      
                         <li><b>AWS Glue Crawler Configuration:</b>
                      <ul>

                          <li>Configured AWS Glue Crawlers to automatically discover and catalog metadata about the data stored in S3 buckets. This included inferring the schema and table structure of the data.</li>
                                             
                  
                      </ul></li>

                     
                         <li><b>AWS Glue Data Catalog:</b>
                      <ul>

                          <li>Utilized the AWS Glue Data Catalog as the centralized metadata repository for the data. This catalog contained information about the stock market data tables, their schemas, and locations in S3.</li>
                                              
                  
                      </ul></li>

                      
                    
                         <li><b>AWS Athena Integration:</b>
                      <ul>

                          <li>Set up AWS Athena to query the data stored in S3 buckets. Athena can use the metadata stored in the AWS Glue Data Catalog to facilitate SQL querying.</li>
                          <li>Defined and executed SQL queries to analyze the stock market data.</li>
                          <li>Generated reports, visualizations, and insights from the data using Athena.</li>
                      
                  
                      </ul></li>
                      <br>
                    


                        <br>
                        <div class="subheading mb-3"><b>Youtube Analysis Project using AWS </b></div>
                           <p><i>To be Uploaded Soon</i></p>

                        <br>
                        <br>
                        <div class="subheading mb-3"><b>Data migration from on-prem database to Azure Cloud</b></div>
                           <p>
                            <ul>

                         
                            <li><b>Leveraged Azure stack for ETL Pipelines and Analytics:</b>
                            <ul>
                                <li>Utilized a suite of Azure services, including Azure Data Factory, Databricks, Data Lake Storage Gen2, Synapse Analytics, Key Vault, Active Directory, and PowerBI.</li>
                                <li>Employed these services to design, develop, and execute end-to-end ETL (Extract, Transform, Load) pipelines and conducted advanced analytics on data.</li>

                            </ul></li>
                            
                               <li> <b>Resource Group Management and Integration:</b>
                            <ul>

                                <li>Created and managed resource groups to logically organize and streamline Azure resources for efficient administration.</li>
                                <li>Configured self-hosted runtime integrations, enabling seamless communication between Azure resources and an on-premise database.</li>
                                <li>Facilitated smooth data flow between cloud and on-premises environments.</li>
                        
                            </ul></li>

                            
                            <li><b>Data Extraction and Storage with Azure Data Factory (ADF):</b>
                         <ul>

                             <li>Designed and implemented data pipelines and activities within Azure Data Factory (ADF).</li>
                             <li>Leveraged ADF to efficiently extract specific tables and datasets from the on-premise database.</li>
                             <li>Stored extracted data in Azure Data Lake Storage Gen2, ensuring accessibility and data integrity.</li>
                     
                         </ul>

                      
                           <li> <b>Data Transformation with Databricks:</b>
                         <ul>

                             <li>Utilized Azure Databricks for comprehensive data transformation.</li>
                             <li>Processed raw, unstructured data from diverse sources, transforming it into curated, high-quality data suitable for advanced analytics.</li>
                         
                     
                         </ul></li>

                      
                         <li><b>Data Loading with Azure Synapse Analytics:</b>
                      <ul>

                          <li>Employed Azure Synapse Analytics for data loading and management.</li>
                          <li>Loaded clean and transformed data into Azure Synapse Analytics, preparing it for in-depth analytical queries and reporting.</li>
                      
                  
                      </ul></li>

                     
                         <li><b>Interactive Dashboard Creation with PowerBI:</b>
                      <ul>

                          <li>Utilized PowerBI to craft interactive, user-friendly dashboards.</li>
                          <li>Enabled users to explore data visually and derive valuable insights through rich and intuitive data visualization.</li>
                      
                  
                      </ul></li>

                      
                    
                         <li><b>Active Directory and Key Vault for Security and Governance:</b>
                      <ul>

                          <li>Implemented Azure Active Directory for robust access control and authentication, ensuring secure interactions with Azure resources.</li>
                          <li>Utilized Azure Key Vault to securely store and manage sensitive credentials and secrets. </li>
                          <li>These measures enhanced data security and supported comprehensive monitoring and data governance practices.</li>
                      
                  
                      </ul></li>






                    </ul>
                           </p>
							

                            
                        </div>


                       









                       
                    </div>






                    <br>
                        <div class="subheading mb-3"><b>Building ETL (Extract, Transform, Load) pipelines for Tokyo Olympic Datasets</b></div>
                        <p><a href="https://github.com/ChirravuriChaitanya/tokyo-olympic-azure-data-engineering-project">View Project</a></p>
                        <p>
                            <ul>

                         
                            <li><b>Learning Azure Tools for Tokyo Olympic Data ETL:</b>
                            <ul>
                                <li>Acquired proficiency in Azure tools including Azure Data Factory, Databricks, Data Lake, and Synapse for the purpose of building ETL (Extract, Transform, Load) pipelines.</li>
                                <li>Applied this knowledge to work with Tokyo Olympic data, demonstrating the practical application of these tools.</li>

                            </ul></li>
                            
                               <li> <b>Resource Group Management and Data Extraction:</b>
                            <ul>

                                <li>Established and managed resource groups in Azure for effective organization and resource management.</li>
                                <li>Configured self-hosted runtime integrations, enabling seamless communication between Azure resources and an on-premise database.</li>
                                <li>Demonstrated expertise in handling external data sources, ensuring data retrieval efficiency.</li>
                        
                            </ul></li>

                            
                            <li><b>Data Extraction and Storage with Azure Data Factory (ADF):</b>
                         <ul>

                             <li>Leveraged Azure Data Factory (ADF) to create pipelines and activities for extracting specific tables or datasets from the source database.</li>
                             <li>Utilized ADF as a key component in the data extraction process, ensuring seamless data transfer.</li>
                             <li>Stored the extracted data in Azure Data Lake Storage Gen2, providing a secure and scalable storage solution.</li>
                     
                         </ul>

                      
                           <li> <b>Data Transformation with Databricks:</b>
                         <ul>

                             <li>Harnessed the power of Azure Databricks, employing PySpark for data transformation.</li>
                             <li>Transformed raw data into cleaner, well-structured data, enhancing data quality and readiness for analysis.</li>
                             <li>Demonstrated proficiency in utilizing Databricks for data wrangling and refinement.</li>
                         
                     
                         </ul></li>

                      
                         <li><b>Data Loading with Azure Synapse Analytics:</b>
                      <ul>

                          <li>Utilized Azure Synapse Analytics for data loading and management, ensuring efficient data ingestion.</li>
                          <li>Loaded the clean and transformed data into the database within Azure Synapse Analytics.</li>
                          <li>Leveraged Synapse Analytics for query analytics, enabling the creation of visual graphs and charts for data visualization.</li>
                          <li>Demonstrated the ability to derive valuable insights through query analytics.</li>
                      
                  
                      </ul></li>             
                        </ul>
                           </p>

                           
							

                           <br>
                           <div class="subheading mb-3"><b>Data Modelling</b></div>
                           <p><a href="https://github.com/ChirravuriChaitanya/DataModelling">View Project</a></p>
                           <p>
                            <ul>

                         
                            <li><b>Overview and Dataset Selection:</b>
                            <ul>
                                <li>The project revolves around the analysis of the dvdrental dataset, which is readily available on the PostgreSQL website.</li>
                                <li>The first step involves downloading this dataset and preparing it for further processing.</li>

                            </ul></li>
                            
                               <li> <b>Data Loading into PostgreSQL:</b>
                            <ul>

                                <li>Following dataset selection, the data is downloaded and loaded into a PostgreSQL database.</li>
                                <li>This process ensures that the dataset is accessible and can be manipulated for subsequent analysis.</li>
                                                       
                            </ul></li>

                            
                            <li><b>Connecting Python to PostgreSQL with psycopg2:</b>
                         <ul>

                             <li>To work with the data effectively, the psycopg2 library is employed to establish a connection between Python and PostgreSQL.</li>
                             <li>This connection facilitates seamless interaction with the database from within Python.</li>
                            
                     
                         </ul>

                      
                           <li> <b>Data Model Design in STAR Schema:</b>
                         <ul>

                             <li>A pivotal part of the project is the design of a data model. In this case, a relational modeling approach is chosen.</li>
                             <li>The goal is to restructure the dataset into a STAR Schema, a common data warehousing model that simplifies data retrieval and analysis.</li>
                                                    
                     
                         </ul></li>

                      
                         <li><b>Using psycopg2 for Database Operations:</b>
                      <ul>

                          <li>The psycopg2 library is instrumental in this process and is used for various tasks.</li>
                          <li>Key actions include initializing the connection, creating the database, and initializing a cursor to execute queries.</li>
                                              
                  
                      </ul></li>
                      <li> <b>Query Execution and Database Operations:</b>

                      <ul>

                        <li>The initialized cursor is employed to execute SQL queries, perform data manipulations, and carry out operations on the database.</li>
                        <li>This phase involves crafting and executing queries to transform and restructure the dataset as per the STAR Schema design.</li>
                                               
                
                    </ul></li>

                 
                    <li><b>Closing Cursor and Connection:</b>
                 <ul>

                     <li>After the necessary database operations are completed, it's essential to close the cursor and connection.</li>
                     <li>This ensures efficient resource management and proper termination of the database connection.</li>
                                         
             
                 </ul></li>

                     
                        
                    </ul>
                           </p>  
                           <br>
                           <br>
                           <br>




                           <h3 class="mb-0">Full Stack Learnings</h3>
                           <p>Next Coming Up! <b>Blog Clone Project</b></p>










                        </div>
          

                       
                    </div>
          
                     

                     
                 </div>



                    
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            
                            
                    </div>
                </div>
            </section>






            <hr class="m-0" />
            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h2 class="mb-5">Education</h2>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">George Mason University</h3>
                            <div class="subheading mb-3">Master of Science</div>
                            <div>Computer Science </div>
                            <p>GPA: 3.40</p>
			<p><b> CourseWork:</b> Theory/Application Data Mining, Mining Mass Datasets MapReduce, Software Model/Architechural Deisgn, Software Engineering for World Wide Web, Mobile Immersive Computing, Database Systems, Computer Systems and System Programming, Component-Based Software Development, Mathematical Foundations of CS, Analysis of Algorithms I</p>
                        </div>
                        <div class="flex-shrink-0"><span class="text-primary">August 2021 - May 2023</span></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between">
                        <div class="flex-grow-1">
                            <h3 class="mb-0">Mahindra University</h3>
                            <div class="subheading mb-3">Bachelor of Science</div>
			 <div>Computer Science </div>
                            <p>GPA: 7.29</p>
		<p><b> CourseWork:</b> Artificial Intelligence, Cloud Computing, 
Computer Architectural Design, Computer Communications Networks, Cryptography & Information Security, 
Data Structures & Algorithms, Mobile Communications & Computing, Object Oriented Programming Using Java, 
Operating Systems, 
Entrepreneurship, IPR & LAW, Design Thinking, 
Corporate Management & Finance, Cinema & Philosophy </p>
				
			
                        </div>

                        <div class="flex-shrink-0"><span class="text-primary">August 2014 - May 2018</span></div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Skills-->
            <section class="resume-section" id="skills">
                <div class="resume-section-content">
                    <h2 class="mb-5">Skills</h2>
                    <p><ul><li><b>Programming Languages:</b> Python, Java, JavaScript, SQL, HTML/CSS </li>
<li><b>Databases :</b> MySQL, PostgreSQL, MongoDB, Snowflake, Cassandra</li>
<li><b>Big Data Technologies : </b> Hadoop, HDFS-MapReduce, Hive, Spark, PySpark, Spark Streaming</li>
<li><b> Frameworks :</b>Apache Kafka, Airflow, Django, Flask, Fast API, Rest API AngularJS </li>
<li><b>Cloud Technologies :</b>  AWS (EC2, S3, RDS, RedShift, SNS, SQS, CloudWatch, Glue,Kinesis)<br/>
                               &emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &ensp; Azure(Data Factory, Data Lake, Lake House, Databricks, Key Vault, Active Directory)<br/>
                               &emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; &ensp;GCP Big Query
</li>
<li><b>Visualization Tools & CI/CD :</b>Power BI, Matplotlib, Tableau, Docker, Git, Kubernetes</li></ul></p>
                    <div class="subheading mb-3">Workflow</div>
                    <ul class="fa-ul mb-0">
                       
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Cross Functional Teams
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Agile Development & Scrum
                        </li>
			<li>
                            <span class="fa-li"><i class="fas fa-check"></i></span>
                            Kanban
                        </li>
                    </ul>
                </div>
            </section>
            <hr class="m-0" />

  <!-- Volunteer Experience-->
            <section class="resume-section" id="volunteer">
                <div class="resume-section-content">
                    <h2 class="mb-5">Volunteer Experience</h2>
                    <p><ul>
			<li>Organized inter-college fests 'Aether' at Mahindra University</li>
			<li>Active member for a NGO to empower women safety</li></ul></p>
                   
                </div>
            </section>
            <hr class="m-0" />

            <!-- Interests-->
            <section class="resume-section" id="interests">
                <div class="resume-section-content">
                    <h2 class="mb-5">Interests</h2>
                    <p>Apart from being a computer science enthuisiast, I enjoy most of my time being outdoors. I enjoy playing basketball, badminton and running.</p>
                    <p class="mb-0">When forced indoors, I follow a number of sci-fi and fantasy genre movies and television shows, I am an aspiring chef, and I spend a large amount of my free time exploring the latest technology advancements across the world.</p>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Awards-->
            <section class="resume-section" id="awards">
                <div class="resume-section-content">
                    <h2 class="mb-5">Achievements & Certifications</h2>
                    <ul class="fa-ul mb-0">
                        
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            Hacker Rank Certification in Python
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - George Mason University - Best Project in Mobile Immersive Computing
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Place - Mahindra University - Deans List 2017
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            2
                            <sup>nd</sup>
                            Place - Mahindra University - Deans List 2016
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>nd</sup>
                            Scholarship receipent for Outstanding Performance - Bhabha Atomic Research Centre
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            1
                            <sup>st</sup>
                            Distinction - Australian Chemistry - 2010
                        </li>
                        <li>
                            <span class="fa-li"><i class="fas fa-trophy text-warning"></i></span>
                            3
                            <sup>rd</sup>
                            Distinction - Australian Chemistry - 2009
                        </li>
                    </ul>
                </div>
            </section>
        </div>
        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
    </body>
</html>
